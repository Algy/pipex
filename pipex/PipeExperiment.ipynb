{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T10:52:07.227261Z",
     "start_time": "2018-12-19T10:52:02.896751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from os.path import splitext\n",
    "from dateutil.parser import parse as parse_datetime\n",
    "from datetime import datetime\n",
    "from os.path import isfile, isdir, join\n",
    "from typing import Tuple, Iterator, Optional, Any, Dict, List\n",
    "from hashlib import sha1\n",
    "\n",
    "class PAtom:\n",
    "    def __init__(self, value, format):\n",
    "        self.value = value\n",
    "        self.format = format\n",
    "\n",
    "class PRecord:\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            id: str,\n",
    "            channels: Dict[str, PAtom] = None,\n",
    "            timestamp: int,\n",
    "            default_channel: str = 'default'):\n",
    "        self.id = id\n",
    "        self.default_channel = default_channel\n",
    "        self.timestamp = timestamp\n",
    "        self.channels = channels or {}\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        try:\n",
    "            return self.channels[self.default_channel].value\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "    @value.setter\n",
    "    def value(self, v):\n",
    "        self.channels[self.default_channel] = v\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        return self.channels[name]\n",
    "\n",
    "    def get(self, name, default=None):\n",
    "        return self.channels.get(name, default)\n",
    "\n",
    "    def with_channel(self, channel) -> \"PRecord\":\n",
    "        return PRecord(self.id, self.channels, self.timestamp, channel)\n",
    "\n",
    "    def merge(self, **kwargs):\n",
    "        channels = self.channels.copy()\n",
    "        channels.update(kwargs)\n",
    "        return PRecord(\n",
    "            id=self.id,\n",
    "            channels=channels,\n",
    "            timestamp=time.time(),\n",
    "            default_channel=self.default_channel\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<{} id={!r} value={!r} channels={!r}>\".format(\n",
    "            self.id,\n",
    "            self.__class__.__name__,\n",
    "            self.value,\n",
    "            self.channels\n",
    "        )\n",
    "\n",
    "\n",
    "class PipeChain:\n",
    "    def chain_hash(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __bool__(self):\n",
    "        return True\n",
    "\n",
    "    def execute(self, our) -> Iterator[PRecord]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def do(self):\n",
    "        for _ in self.execute({}):\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Source(PipeChain):\n",
    "    def __gt__(self, other) -> \"TransformedSource\":\n",
    "        if not isinstance(other, Transformer):\n",
    "            raise TypeError(\"{!r} is not a Transformer\".format(other))\n",
    "        # Source > PipeChain\n",
    "        return TransformedSource(self, other)\n",
    "\n",
    "    def __iter__(self) -> Iterator[PRecord]:\n",
    "        return self.generate_precords()\n",
    "\n",
    "    def values(self) -> Iterator[Any]:\n",
    "        for precord in self.generate_precords():\n",
    "            yield precord.value\n",
    "\n",
    "    def execute(self, our) -> Iterator[PRecord]:\n",
    "        return self.generate_precords()\n",
    "\n",
    "    def generate_precords(self) -> Iterator[PRecord]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class IteratorSource(Source):\n",
    "    def __init__(self, it: Iterator[PRecord]):\n",
    "        self.it = it\n",
    "\n",
    "    def generate_precords(self) -> Iterator[PRecord]:\n",
    "        return self.it\n",
    "\n",
    "\n",
    "class Sink(PipeChain):\n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, TransformedSource):\n",
    "            return other.with_sink(other)\n",
    "        elif isinstance(other, Source):\n",
    "            return TransformedPipeline(source, IdentityTransformer(), sink)\n",
    "        else:\n",
    "            raise TypeError(\"Not a form of 'Source > Sink''\")\n",
    "\n",
    "    def execute(self, our):\n",
    "        return iter(())\n",
    "\n",
    "    def process(self, our, tr_source: \"TransformedSource\") -> Iterator[PRecord]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ListSourceSink(Source, Sink):\n",
    "    def __init__(self, dest_list: List[PRecord]):\n",
    "        self.dest_list = dest_list\n",
    "\n",
    "    def get_hash(self) -> str:\n",
    "        return str(id(self.dest_list))\n",
    "\n",
    "    def process(self, our, tr_source: \"TransformedSource\") -> Iterator[PRecord]:\n",
    "        dest_list = self.dest_list\n",
    "        for precord in tr_source.generate_precords():\n",
    "            dest_list.append(precord)\n",
    "            yield precord\n",
    "\n",
    "\n",
    "class Transformer(PipeChain):\n",
    "    def __lt__(self, other):\n",
    "        # iterable > transformer\n",
    "        try:\n",
    "            return IteratorSource(iter(other))\n",
    "        except TypeError:\n",
    "            pass\n",
    "        return NotImplemented\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        # iterable > transformer\n",
    "        if hasattr(other, list):\n",
    "            return ListSourceSink(dest_list)\n",
    "        return NotImplemented\n",
    "\n",
    "    def transform(self, precords: Iterator[PRecord]):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def execute(self, our) -> Iterator[PRecord]:\n",
    "        return iter(())\n",
    "\n",
    "\n",
    "class IdentityTransformer(Transformer):\n",
    "    _chain_hash = sha1(b\"IdentityTransformer\").hexdigest()\n",
    "\n",
    "    def transform(self, precords: Iterator[PRecord]) -> Iterator[PRecord]:\n",
    "        return precords\n",
    "\n",
    "    def chain_hash(self) -> str:\n",
    "        return self._chain_hash\n",
    "\n",
    "\n",
    "class TransformedSource(Source):\n",
    "    def __init__(self, source: Source, transformer: Transformer):\n",
    "        self.source = source\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __iter__(self) -> Iterator[PRecord]:\n",
    "        return self.generate_precords()\n",
    "\n",
    "    def chain_hash(self) -> str:\n",
    "        return sha1(\"trsource:{}:{}\".format(\n",
    "            self.source.chain_hash(),\n",
    "            self.transformer.chain_hash()\n",
    "        )).hexdigest()\n",
    "\n",
    "    def with_sink(self, sink: Sink) -> \"TransformedSourceSink\":\n",
    "        return TransformedSourceSink(self.source, self.transformer, sink)\n",
    "\n",
    "    def generate_precords(self) -> Iterator[PRecord]:\n",
    "        return self.transformer.transform(self.source.generate_precords())\n",
    "\n",
    "\n",
    "\n",
    "class TransformedPipeline(Source):\n",
    "    def __init__(self, transformed_source, sink: Sink):\n",
    "        self.transformed_source = transformed_source\n",
    "        self.sink = sink\n",
    "\n",
    "    @property\n",
    "    def source(self):\n",
    "        return self.transformed_source.source\n",
    "\n",
    "    @property\n",
    "    def tranformer(self):\n",
    "        return self.transformed_source.transformer\n",
    "\n",
    "    def chain_hash(self) -> str:\n",
    "        return sha1(\"trsourcesink:{}:{}:{}\".format(\n",
    "            self.source.chain_hash(),\n",
    "            self.transformer.chain_hash(),\n",
    "            self.sink.chain_hash(),\n",
    "        )).hexdigest()\n",
    "\n",
    "    def generate_precords(self) -> Iterator[PRecord]:\n",
    "        return self.transformed_source.generate_precords()\n",
    "\n",
    "    def execute(self, our):\n",
    "        return self.sink.process(our, self.transformed_source)\n",
    "\n",
    "\n",
    "class PStorage:\n",
    "    SAVE_MODES = ('incremental', 'always')\n",
    "    def __init__(self, base_dir='.'):\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def bucket(self, name,\n",
    "               use_batch: bool = False,\n",
    "               batch_size: Optional[int] = None,\n",
    "               flush_interval: float = 1.0,\n",
    "               save_mode: str = 'incremental',\n",
    "              ) -> \"PBucket\":\n",
    "        return PBucket(\n",
    "            storage=self,\n",
    "            scope=(name, ),\n",
    "            use_batch=use_batch,\n",
    "            batch_size=batch_size,\n",
    "            flush_interval=flush_interval,\n",
    "            save_mode=save_mode,\n",
    "        )\n",
    "\n",
    "    def bucket_names(self):\n",
    "        return [\n",
    "            dir\n",
    "            for dir in os.listdir(self.base_dir)\n",
    "            if isdir(dir) and isfile(join(dir, \"pbucket.json\"))\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, name: str) -> \"PBucket\":\n",
    "        return self.bucket(name)\n",
    "\n",
    "\n",
    "class PSourceSink(Source, Sink):\n",
    "    pass\n",
    "\n",
    "\n",
    "from functools import total_ordering\n",
    "\n",
    "@total_ordering\n",
    "class PBucketVersion:\n",
    "    def __init__(self, positions: Tuple[int]):\n",
    "        self.positions = positions\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \".\".join(str(position) for position in self.positions)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.positions < other.positions\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.positions == other.positions\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.positions)\n",
    "\n",
    "    @classmethod\n",
    "    def parse(cls, ver: str):\n",
    "        return cls(tuple(map(int, ver.split(\".\"))))\n",
    "\n",
    "\n",
    "class PBucketMetadata:\n",
    "    def __init__(self, *,\n",
    "                 meta_version: PBucketVersion,\n",
    "                 source_chain_hash: Optional[str],\n",
    "                 source_version: int,\n",
    "                 latest_record_timestamp: int):\n",
    "        self.meta_version = meta_version\n",
    "        self.source_chain_hash = source_chain_hash\n",
    "        self.source_version = source_version\n",
    "        self.latest_record_timestamp = latest_record_timestamp\n",
    "\n",
    "    \n",
    "    def to_json(self):\n",
    "        return {\n",
    "            'version': self.version,\n",
    "            'meta_version': str(self.meta_version),\n",
    "            'source_chain_hash': self.source_chain_hash,\n",
    "            'source_version': self.source_version,\n",
    "            'latest_record_timestamp': self.latest_record_timestamp,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, data):\n",
    "        return cls(\n",
    "            meta_version=PBucketVersion.parse(data['meta_version']),\n",
    "            source_chain_hash=data['source_chain_hash'],\n",
    "            source_version=data['source_version'],\n",
    "            latest_record_timestamp=data['latest_record_timestamp'],\n",
    "        )\n",
    "    \n",
    "\n",
    "    @classmethod\n",
    "    def initial(cls, meta_version: PBucketVersion):\n",
    "        return cls(\n",
    "            meta_version=meta_version,\n",
    "            source_chain_hash=None,\n",
    "            source_version=1,\n",
    "            latest_record_timestamp=0,\n",
    "        )\n",
    "\n",
    "   \n",
    " \n",
    "class PBucket(PSourceSink):\n",
    "    META_VERSION = PBucketVersion.parse('0.0.1')\n",
    "\n",
    "    def __init__(self, storage: PStorage,\n",
    "                 scope: Tuple[str],\n",
    "                 use_batch: bool,\n",
    "                 batch_size: Optional[int],\n",
    "                 flush_interval: float,\n",
    "                 save_mode: str):\n",
    "        self.storage = storage\n",
    "        self.scope = scope\n",
    "        self.use_batch = use_batch\n",
    "        self.batch_size = batch_size\n",
    "        self.flush_interval = flush_interval\n",
    "        self.save_mode = save_mode\n",
    "        \n",
    "        self.logger = logging.getLogger(\"PBucket({!r})\".format(self.directory_name))\n",
    "        self._last_flush_time = None\n",
    "\n",
    "    @property\n",
    "    def directory_name(self):\n",
    "        return join(self.storage.base_dir, *self.scope)\n",
    "\n",
    "    @property\n",
    "    def meta_name(self):\n",
    "        return join(self.directory_name, \"pbucket.json\")\n",
    "\n",
    "    @property\n",
    "    def meta_tmp_name(self):\n",
    "        return join(self.directory_name, \"pbucket.json.tmp\")\n",
    "\n",
    "    @property\n",
    "    def data_directory_name(self):\n",
    "        return join(self.directory_name, \"data\")\n",
    "\n",
    "    # load\n",
    "    def generate_precords(self) -> Iterator[PRecord]:\n",
    "        self._ensure_pbucket_dir()\n",
    "        directory_name = self.directory_name\n",
    "        data_directory_name = self.data_directory_name\n",
    " \n",
    "        for file_name in os.listdir(data_directory_name):\n",
    "            id_str, file_ext = splitext(file_name)\n",
    "            if file_ext != '.json':\n",
    "                continue\n",
    "            precord = self._load_precord(id_str)\n",
    "            yield precord\n",
    "\n",
    "    def _load_precord(self, id: str):\n",
    "        data_directory_name = self.data_directory_name\n",
    "        file_name = join(self.data_directory_name, id + \".json\") \n",
    "        with open(file_name) as f:\n",
    "            d = json.load(f)\n",
    "        id = d['id']\n",
    "        default_channel = d['default_channel']\n",
    "        channel_names = d['channel_names']\n",
    "        channel_formats = d['channel_formats']\n",
    "        timestamp = d['timestamp']\n",
    "        data = d['data']\n",
    "        channels = {}\n",
    "        for channel_name, format in zip(channel_names, channel_formats):\n",
    "            value = None\n",
    "            if format == 'data':\n",
    "                value = data.get(channel_name)\n",
    "            else:\n",
    "                channel_dir_name = open(join(directory_name, channel_name))\n",
    "                if format == 'image':\n",
    "                    ext = '.png'\n",
    "                elif format == 'numpy.ndarray':\n",
    "                    ext = '.npz'\n",
    "                elif format == 'text':\n",
    "                    ext = '.txt'\n",
    "                else:\n",
    "                    ext = '.dat'\n",
    "                value_file_name = join(channel_dir_name, id + ext)\n",
    "\n",
    "                if format == 'image':\n",
    "                    with Image.open(value_file_name) as img:\n",
    "                        value = np.array(img)\n",
    "                elif format == 'numpy.ndarray':\n",
    "                    value = np.load(value_file_name)\n",
    "                elif format == 'text':\n",
    "                    with open(value_file_name, \"r\") as f:\n",
    "                        value = f.read()\n",
    "                else:\n",
    "                    with open(value_file_name, \"rb\") as f:\n",
    "                        value = f.read()\n",
    "            channels[name] = PAtom(value, format)\n",
    "        return PRecord(\n",
    "            id=id,\n",
    "            channels=channels,\n",
    "            timestamp=timestamp,\n",
    "            default_channel=default_channel\n",
    "        )\n",
    "\n",
    "    def _save_precord(self, precord: PRecord):\n",
    "        directory_name = self.directory_name\n",
    "        data_directory_name = self.data_directory_name\n",
    "        data_file_name = join(data_directory_name, precord.id + \".json\")\n",
    "        channel_names = list(precord.channels.keys())\n",
    "        data = {}\n",
    "        d = {\n",
    "            \"id\": precord.id,\n",
    "            \"default_channel\": precord.default_channel,\n",
    "            \"channel_names\": channel_names,\n",
    "            \"channel_formats\": [precord.channels[name].format for name in channel_names],\n",
    "            \"timestamp\": precord.timestamp,\n",
    "            \"data\": data,\n",
    "        }\n",
    "        \n",
    "        id = precord.id\n",
    "        for channel_name, patom in precord.channels.items():\n",
    "            value, format = patom.value, patom.format\n",
    "            if format == 'data':\n",
    "                data[channel_name] = value\n",
    "                continue\n",
    "            elif format == 'image':\n",
    "                ext = '.png'\n",
    "            elif format == 'numpy.ndarray':\n",
    "                ext = '.npz'\n",
    "            elif format == 'text':\n",
    "                ext = '.txt'\n",
    "            else:\n",
    "                ext = '.dat'\n",
    "\n",
    "            channel_dir_name = open(join(directory_name, channel_name))\n",
    "            value_file_name = join(channel_dir_name, id + ext)\n",
    "\n",
    "            if format == 'image':\n",
    "                Image.fromarray(value).save(value_file_name)\n",
    "            elif format == 'numpy.ndarray':\n",
    "                np.savez_compressed(value_file_name)\n",
    "            elif format == 'text':\n",
    "                with open(value_file_name, \"w\") as f:\n",
    "                    f.write(value)\n",
    "            else:\n",
    "                with open(value_file_name, \"wb\") as f:\n",
    "                    f.write(value)\n",
    "\n",
    "    def _save_precord_with_flush(self, precord: PRecord, metadata: PBucketMetadata):\n",
    "        self._save_precord(precord)\n",
    "        metadata.latest_record_timestamp = max(\n",
    "            metadata.latest_record_timestamp,\n",
    "            precord.timestamp,\n",
    "        )\n",
    "        now = time.time()\n",
    "        if self._last_flush_time is None or now - self._last_flush_time > self.flush_interval:\n",
    "            self._flush_metadata(metadata)\n",
    "            self._last_flush_time = now\n",
    "\n",
    "    def process(self, our, tr_source: \"TransformedSource\") -> Iterator[PRecord]:\n",
    "        self._ensure_pbucket_dir()\n",
    "        metadata = self._load_metadata()\n",
    "\n",
    "        use_existing = False\n",
    "        timestamp\n",
    "        if self.save_mode == 'incremental':\n",
    "            source_chain_hash = tr_source.chain_hash()\n",
    "            timestamp\n",
    "            if source_chain_hash != metadata.source_chain_hash:\n",
    "                pass\n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "        try:\n",
    "            if self.use_batch:\n",
    "                if self.batch_size is None:\n",
    "                    # full batch\n",
    "                    for precord in tr_source.genenrate_precords():\n",
    "                        self._save_precord_with_flush(precord, metadata)\n",
    "                    yield from self.genenrate_precords()\n",
    "                else:\n",
    "                    # mini batch\n",
    "                    mini_batch = []\n",
    "                    for index, precord in enumerate(tr_source.genenrate_precords()):\n",
    "                        self._save_precord_with_flush(precord, metadata)\n",
    "                        mini_batch.append(precord)\n",
    "                        if (index + 1) % self.batch_size:\n",
    "                            yield from mini_batch\n",
    "                            mini_batch.clear()\n",
    "                    if mini_batch:\n",
    "                        yield from mini_batch\n",
    "                        mini_batch.clear()\n",
    "            else:\n",
    "                # stream\n",
    "                for precord in tr_source.genenrate_precords():\n",
    "                    self._save_precord_with_flush(precord, metadata)\n",
    "                    yield precord\n",
    "        finally:\n",
    "            self._flush_metadata(metadata)\n",
    "    \n",
    "    \n",
    "    def _infer_format_from_type(channel_name: str, value: Any) -> str:\n",
    "        if isinstance(value, np.ndarray):\n",
    "            if channel_name.startswith('image') or channel_name.startswith('img'):\n",
    "                return 'image'\n",
    "            else:\n",
    "                return 'np.ndarray'\n",
    "        elif isinstance(value, (type(None), str, int, float, bool, list)):\n",
    "            return \"data\"\n",
    "        else:\n",
    "            return \"blob\"\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        meta_name = self.meta_name\n",
    "        with open(meta_name) as f:\n",
    "            data = json.load(f)\n",
    "            return PBucketMetadata.from_json(data)\n",
    "\n",
    "    def _flush_metadata(self, metadata: PBucketMetadata):\n",
    "        meta_name = self.meta_data\n",
    "        meta_tmp_name = self.meta_tmp_name\n",
    "\n",
    "        if isfile(meta_tmp_name):\n",
    "            raise RuntimeError(\"{} exists! Some other process might be modifying this bucket.\".format(meta_tmp_name))\n",
    "\n",
    "        with open(meta_tmp_name, \"w\") as f:\n",
    "            f.write(json.dumps(metadata.to_json()))\n",
    "        os.rename(meta_tmp_name, meta_name)\n",
    "\n",
    "    def _ensure_pbucket_dir(self):\n",
    "        directory_name = self.directory_name\n",
    "        data_directory_name = self.data_directory_name\n",
    "        meta_name = join(directory_name, \"pbucket.json\")\n",
    "\n",
    "        self._ensure_dir(directory_name)\n",
    "        self._ensure_dir(data_directory_name)\n",
    "\n",
    "        if not isfile(meta_name):\n",
    "            self._flush_metadata(PBucketMetadata.initial(self.META_VERSION))\n",
    "\n",
    "    def _ensure_dir(self, name):\n",
    "        if not isdir(name):\n",
    "            os.makedirs(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T11:27:31.687359Z",
     "start_time": "2018-12-19T11:27:31.684634Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T11:29:01.979264Z",
     "start_time": "2018-12-19T11:29:01.976569Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T10:49:10.662924Z",
     "start_time": "2018-12-19T10:49:10.434463Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T10:31:04.366078Z",
     "start_time": "2018-12-19T10:31:04.362413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1.json'.rstrip('.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T10:31:11.742054Z",
     "start_time": "2018-12-19T10:31:11.739361Z"
    }
   },
   "outputs": [],
   "source": [
    "v = '1.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T10:31:27.268202Z",
     "start_time": "2018-12-19T10:31:27.265152Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T10:31:31.893211Z",
     "start_time": "2018-12-19T10:31:31.888973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
